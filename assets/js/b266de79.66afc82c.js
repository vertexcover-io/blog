"use strict";(self.webpackChunkblog_vertexcover=self.webpackChunkblog_vertexcover||[]).push([[518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"strot-is-a-review-parser","metadata":{"permalink":"/strot-is-a-review-parser","source":"@site/blog/2025-07-28-strot-review-parser.mdx","title":"Strot - The Review Parser","description":"This is a breakdown into what went into making Strot - The Review Parser.","date":"2025-07-28T00:00:00.000Z","tags":[],"readingTime":4.96,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"strot-is-a-review-parser","title":"Strot - The Review Parser","date":"2025-07-28T00:00:00.000Z","draft":false},"unlisted":false,"nextItem":{"title":"Teaching Claude Code to Work Independently","permalink":"/claude-code-context-engineering-v2"}},"content":"import ComparisonTable from \'../src/components/ComparisonTable\';\\nimport NumberedList from \'../src/components/NumberedList\';\\nimport TLDR from \'../src/components/TLDR\';\\nimport HierarchySection from \'../src/components/HierarchySection\';\\nimport SubSection from \'../src/components/SubSection\';\\n\\n\\nThis is a breakdown into what went into making Strot - The Review Parser. \\n\\n<TLDR>\\nStrot works in two steps:\\n\\n<NumberedList items={[\\n  \\"Intercepts the right ajax calls from the website based on both visual analysis and via captured HAR logs\\",\\n  \\"Uses those calls to fetch the reviews subsequently. So, getting top 100 reviews doesn\'t need to scrape the website AGAIN.\\"\\n]} />\\n</TLDR>\\n\\n\\n{/* truncate */}\\n\\n\\n## What if ... ?\\n\\nWhen is comes to scraping reviews, things CAN BE slightly different than running playwright scripts. That\'s the idea we started with! \\n\\nIt stems from the insights on browsing shopify. Since, shopify has plugin ecosystem, a lot of reviews come from plugins. They give the reviews in html / json form via api that the page renders. \\nSince, reviews - if they are worthwhile - would be in 100s for a given product, it would be very unlikely that someone with good web-dev practices will send all of them on one shot. There MUST be a second call (pagination) that requests for further reviews for the given product. \\n\\nWhat if ... we could scrape reviews via intercepting AJAX calls that are made from the browser? \\n\\n\\n## The Genesis \\n\\nHence, an experiement is born - called `strot`. In sanskrit it means source. We are trying to get to the ajax calls that gets us reviews. \\n\\nThe review scraping problem sits at a unique position. There are always MANY reviews which either causes cost ballooning no matter which existing approach you take. \\n\\n<ComparisonTable \\n  data={[\\n    {\\n      feature: \'Cost\',\\n      playwright: { text: \'Infrastructure costs for automation\', score: \'medium\' },\\n      llmScraper: { text: \'Pay per page + LLM inference\', score: \'poor\' },\\n      strot: { text: \'One-time API discovery\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Speed\',\\n      playwright: { text: \'Slow - full page loads\', score: \'poor\' },\\n      llmScraper: { text: \'Slow - LLM processing per page\', score: \'poor\' },\\n      strot: { text: \'Fast - direct API calls\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Reliability\',\\n      playwright: { text: \'Breaks with UI changes\', score: \'poor\' },\\n      llmScraper: { text: \'Depends on LLM accuracy\', score: \'medium\' },\\n      strot: { text: \'Most reliable - uses actual APIs\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Scalability\',\\n      playwright: { text: \'Limited by browser resources\', score: \'poor\' },\\n      llmScraper: { text: \'Limited by LLM rate limits\', score: \'poor\' },\\n      strot: { text: \'Highly scalable - standard HTTP\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Maintenance\',\\n      playwright: { text: \'High - UI changes break scripts\', score: \'poor\' },\\n      llmScraper: { text: \'Medium - prompt engineering needed\', score: \'medium\' },\\n      strot: { text: \'Low - APIs rarely change\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Intelligence Required\',\\n      playwright: { text: \'High - manual scraper\', score: \'poor\' },\\n      llmScraper: { text: \'High - per page analysis\', score: \'poor\' },\\n      strot: { text: \'One-time - for API discovery\', score: \'medium\' }\\n    },\\n    {\\n      feature: \'Volume Handling\',\\n      playwright: { text: \'Poor - loads full pages\', score: \'poor\' },\\n      llmScraper: { text: \'Expensive - each page costs $\', score: \'poor\' },\\n      strot: { text: \'Excellent - pagination via API\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Setup Complexity\',\\n      playwright: { text: \'Medium - browser automation\', score: \'medium\' },\\n      llmScraper: { text: \'Low - plug and play\', score: \'good\' },\\n      strot: { text: \'Low - plug and play\', score: \'good\' }\\n    },\\n    {\\n      feature: \'Uniqueness\',\\n      playwright: { text: \'Common approach\', score: \'poor\' },\\n      llmScraper: { text: \'Common approach\', score: \'poor\' },\\n      strot: { text: \'Unique to us\', score: \'good\' }\\n    }\\n  ]}\\n  columns={[\'playwright\', \'llmScraper\', \'strot\']}\\n  columnHeaders={[\'Playwright\', \'LLM Scraper\', \'Strot (Ours)\']}\\n/>\\n\\nSo, IF we could find the ajax call \u2192 we only have to spend time figuring out the API call. and voila! You can simply call API - this is cheapest, fastest, most reliable approach of all. \\n\\nAnd the unique to us. Hence, we took a bite. \\n\\nBut the challenge is which api call is most relevnt for scraping reviews ? \\nwe solve this by capturing screenshot, visually analysing if review is preseng in the screenshot. Then we find a matching ajax call that has a similar content. Voila! \\n\\n{/* <todo - insert screenshot for the ajazx call matching - either matches 90% or does not match at all > */}\\n\\n\\n## The codegen \\n\\nWe internally represent all the relevant information as json. This helps us to generate http client in any language of choice. Calling this API endpoint in succession will give all the reviews.\\n\\n\\n<HierarchySection title=\\"Iteratively Improving via Error Analysis and Evals\\">\\n\\nEvals were central to us while building this. We had three tiered system for evals that served us well.\\n\\n<NumberedList items={[\\n  \\"Does the analysis identify the right ajax call?\\",\\n  \\"Is the pagination strategy detection done correctly?\\",\\n  \\"Is the http api (codegen) able to get all the reviews?\\"\\n]} />\\n\\n<SubSection title=\\"Error Analysis\\">\\n\\nThis was the most consuming element for us until we vibe-coded initial version of this dashboard to see the logs as data.\\n\\nHere is a quick peek over the dashboard we made to do the error analysis:\\n\\nThis shows us the cost and token usage:\\n<a href=\\"/img/strot-dashboard-analysis.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-analysis.png\\" alt=\\"Strot dashboard cost and token analysis\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\nThis shows us the step by step observability into what is happening:\\n<a href=\\"/img/strot-dashboard-step.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-step.png\\" alt=\\"Strot dashboard step by step analysis\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n\\n</SubSection>\\n\\n<SubSection title=\\"Pagination\\">\\n\\n<a href=\\"/img/strot-dashboard-pagination.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-pagination.png\\" alt=\\"Strot dashboard pagination\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n</SubSection>\\n \\n<SubSection title=\\"Codegen\\">\\n\\n<a href=\\"/img/strot-dashboard-codegen.png\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">\\n  <img src=\\"/img/strot-dashboard-codegen.png\\" alt=\\"Strot dashboard codegen\\" style={{cursor: \'pointer\'}} />\\n</a>\\n\\n</SubSection>\\n\\n</HierarchySection>\\n\\n\\nWe are also learning. This was our attempt at scraping the webpage and using the visual understanding of image models to get to the reviews faster. if you feel this can be improved, feel free to share those with us on [github-issues](https://github.com/vertexcover-io/strot/issues).\\n\\n\\n## Roadmap \\n\\n<NumberedList items={[\\n  \\"We are already on a path to make this a generic scraper. Currently, our evals sets show the tracking progress on various websites. It will be released soon.\\",\\n  \\"if you would like us to host this as a service for you, we are more than happy to. Come chat with us at [contact email]\\"\\n]} />"},{"id":"claude-code-context-engineering-v2","metadata":{"permalink":"/claude-code-context-engineering-v2","source":"@site/blog/2025-07-26-claude-code.mdx","title":"Teaching Claude Code to Work Independently","description":"<TldrCallout","date":"2025-07-21T00:00:00.000Z","tags":[{"inline":false,"label":"Context Engineering","permalink":"/tags/context-engineering","description":"Context Engineering"}],"readingTime":6.94,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"claude-code-context-engineering-v2","title":"Teaching Claude Code to Work Independently","date":"2025-07-21T00:00:00.000Z","authors":["abeeshake"],"tags":["context-engineering"],"draft":false},"unlisted":false,"prevItem":{"title":"Strot - The Review Parser","permalink":"/strot-is-a-review-parser"},"nextItem":{"title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","permalink":"/ai-agent-end-to-end-automated-tests-mobile"}},"content":"import TldrCallout from \'@site/src/components/TldrCards/TldrCallout\';\\nimport TipsSection from \'@site/src/components/TipsSection\';\\nimport MinimalDetails from \'@site/src/components/MinimalDetails\';\\nimport ScriptAttribution from \'@site/src/components/ScriptAttribution\';\\nimport CalendlyButton from \'@site/src/components/CalendlyButton\';\\n\\n\\n\\n<TldrCallout \\n  problem=\\"You\'re trapped micromanaging Claude Code instead of building\\"\\n  solution=\\"Train Claude Code once, then let it work autonomously\\"\\n  how=\\"Strategic context engineering: CLAUDE.md files, systematic workflows, and learning capture\\"\\n  benefit=\\"Claude Code becomes your autonomous teammate\\"\\n/>\\n\\nHere is [CLAUDE.md](https://gist.github.com/tripathi456/bfaf9add4b70bff131cd574c2f93cfac)\\n\\n\\n{/* truncate */}\\n\\n## Day 0: Just moved from cursor to claude code\\n\\n*It\'s 11 PM on a Tuesday. My token budget just hit zero. Claude Code is asking me the same question for the fourth time: \\"What coding style does this project use?\\" I\'ve spent 6 hours being a glorified copy-paste machine, explaining the same context over and over.*\\n\\n*Sound familiar?*\\n\\nThat night, I realized something critical: **Claude Code is teachable**. But I was the worst teacher on Earth.\\n\\n## Day 1: Fighting Claude Code (The Problem)\\n\\nEvery conversation was Groundhog Day:\\n\\n- \\"What\'s our testing framework again?\\"\\n- \\"How do we name files in this project?\\"  \\n- \\"What\'s the deployment process?\\"\\n\\nI was Claude Code\'s personal Wikipedia. **This had to change.**\\n\\n## Day 2: The Teaching Template (The Mentor)\\n\\n> What if Claude Code could remember your project like a team member who\'s been there for years?\\n\\nInstead of explaining everything every time, I wrote a template. The gist is:\\n\\n```md\\n# Communication: Be concise, reference past learnings from docs/work/\\n# File Naming: YYYY-MM-DD-[001]-[category]-[summary].md  \\n# Never code without checking docs/work/ for similar past solutions\\n```\\n\\nThat\'s it. **This eliminated hours of repetitive context.**\\n\\n<MinimalDetails summary=\\"See the prompt \u2192\\">\\n\\n```md\\n# Project: [Your Project Name]\\n\\n## Tech Stack & Tooling\\n- **Language**: Python 3.11+\\n- **Package Manager**: `uv` (use `uv add <dependency>`, `uv run <script>`)\\n- **Testing**: pytest with coverage\\n- **Linting**: ruff + mypy\\n\\n## Systematic File Naming\\nFormat: `YYYY-MM-DD-[001-999]-[category]-[four-word-summary].md`\\nFolder: `docs/work/`\\nCategories: `bug` | `feature` | `task` | `research` | `learnings`\\n\\nExamples:\\n- `2025-07-18-001-feature-user-authentication-system.md`\\n- `2025-07-18-002-bug-database-connection-timeout.md`\\n\\n## Communication Style\\n- **Concise**: No fluff, direct responses\\n- **Evidence-based**: Show, don\'t just tell\\n- **Contextual**: Reference past learnings from `docs/work/`\\n\\n## Planning Protocol\\n1. **Context Gathering**: Check `docs/work/` for relevant past decisions\\n2. **Assumption Documentation**: Explicit assumptions in plan files\\n3. **Execution Gate**: Only proceed after planning is complete\\n```\\n\\n</MinimalDetails>\\n\\n**The transformation was instant.** Claude Code started referencing past decisions, avoiding repeated mistakes, and building on previous work. **It finally felt like working with a teammate.**\\n\\n## Day 3: The Learning Moment - 30% context - (The Training)\\n\\n> What if Claude Code could learn from every mistake and never repeat it?\\n\\nWhen your context hits 30%, you have one chance to crystallize everything learned. Miss it, and you go back to day 1.\\n\\n\\n\\n<MinimalDetails summary=\\"See the prompt \u2192\\">\\n\\n```md\\n\\nWhen context drops below 30%: \\n\\n1. Document every decision made\\n2. List what failed (with code snippets)  \\n3. Note what worked brilliantly\\n4. Write handoff notes for next session\\n\\nUse the `Systematic File Naming` given above.\\n```\\n\\n</MinimalDetails>\\n\\n\\n## Day 4: The Autonomous Engineer (The Victory)\\n\\nI saw this tweet - [@svs used Claude Code as an MCP client to write an MCP server](https://x.com/_svs_/status/1928753160337637726). \\n\\nSomething magical discovered: **When put in verifiable workflows, Claude Code started working much better!**:\\n\\n- Write code \u2192 Run tests \u2192 Fix failures \u2192 Repeat\\n- Build feature \u2192 Deploy to staging \u2192 Check logs \u2192 Iterate  \\n- Analyze data \u2192 Generate insights \u2192 Verify against sources \u2192 Summarize\\n\\n**I wasn\'t micromanaging anymore. I was collaborating.**\\n\\n## Stories from other folks\\n\\nMet awesome folks at [Fifth Elephant Conference](https://hasgeek.com/fifthelephant/2025/) where we shared claude code learnings.\\n\\n<TipsSection contributor=\\"Rajesh\\" contributorUrl=\\"https://www.linkedin.com/in/codingnirvana/\\">\\n\\n### The Token Budget Hack That Doubled My Productivity\\n\\nRajesh was burning through tokens like crazy at his startup. Then he discovered something weird about Claude\'s [5-hour windows](https://support.anthropic.com/en/articles/11145838-using-claude-code-with-your-pro-or-max-plan).\\n\\n**His discovery:** Start sessions at 7 AM instead of 9 AM.\\n\\nWhy? The overlapping windows create a \\"double token zone\\" during peak hours:\\n\\n**Before:** 9am-2pm, 2pm-7pm (standard)  \\n**After:** 7am-12pm, 12pm-5pm, 5pm-10pm (overlapping)\\n\\n**Result:** Between 9am-5pm = **double tokens available**\\n\\n\\n\\n<img src=\\"/img/timeline.svg\\" />\\n\\n\\n\\n### The CSV Strategy That Saved Hours\\n\\nRajesh\'s team was dealing with lots of data analysis requests. CSV files everywhere. Claude kept hitting context limits trying to process raw data.\\n\\n**The breakthrough:** Stop feeding Claude data. Feed it scripts.\\n\\n**Old way:** \\"Here\'s a 10MB CSV, analyze it\\"  \\n**New way:** \\"Write a script to analyze this CSV type, then run it\\"\\n\\n**Why it works:** Scripts are tiny. Results are focused. Claude guides itself using its own analysis output. \\n\\n</TipsSection>\\n\\n<TipsSection contributor=\\"Ashwant\\" contributorUrl=\\"https://www.shelfradar.ai/\\">\\n\\n### The Accidental Discovery That Changed Everything\\n\\nAshwant was debugging a frustrating session. In a moment of rage, he accidentally hit ESC four times.\\n\\n**What happened next blew his mind.**\\n\\nClaude Code showed him a prompt history he\'d never seen before. Every conversation. Every context. **Time travel for developers.**\\n\\n**The magic combo:** ESC + ESC + ESC + ESC = Prompt history navigation\\n\\n**Game changer:** You can resurrect any previous session state instantly.\\n\\n### Claude Code as Your Database Whisperer\\n\\nAt ShelfRadar, Ashwant deployed Claude Code as their internal SQL agent. \\n\\n**The setup:** Claude Code + database schema = autonomous query optimizer\\n\\n**The result:** \\n\\n- Ad-hoc queries refined automatically\\n- Schema changes don\'t break queries  \\n- Claude evolves with your database\\n\\n</TipsSection>\\n\\n**The question isn\'t whether Claude Code is teachable.**  \\n**The question is: Are you ready to become its teacher?**\\n\\nUpdate 1: Claude code introduced hooks. Here is a short collection of hooks that I find useful for my workflow.\\n\\n1. bash script to notify the end of claude code turns. \\n\\n\\n<MinimalDetails summary=\\"Full bash script \u2192\\">\\n```bash\\n#!/bin/bash\\n\\n# Read hook input data\\nINPUT=$(cat)\\nSESSION_DIR=$(basename \\"$(pwd)\\")\\n\\n# Extract message from transcript if available\\nTRANSCRIPT_PATH=$(echo \\"$INPUT\\" | jq -r \'.transcript_path\')\\nif [ -f \\"$TRANSCRIPT_PATH\\" ]; then\\n  MSG=$(tail -10 \\"$TRANSCRIPT_PATH\\" |\\n    jq -r \'select(.message.role == \\"assistant\\") | .message.content[0].text\' |\\n    tail -1 | tr \'\\\\n\' \' \' | cut -c1-60)\\n  MSG=${MSG:-\\"Task completed\\"}\\nelse\\n  MSG=\\"Task completed\\"\\nfi\\n\\n# Show Linux desktop notification (requires notify-send)\\nnotify-send \\"Claude Code ($SESSION_DIR) Done\\" \\"$MSG\\"\\n```\\n\\n</MinimalDetails>\\n\\n<ScriptAttribution\\n  title=\\"Useful Claude Code Hooks & Scripts\\"\\n  description=\\"\\"\\n  links={[\\n    {\\n      url: \\"https://gist.github.com/glennmatlin/fadc41edc3bb9ff68ff9cfa5d6b8aca7\\",\\n      title: \\"Making Claude use uv instead of pip\\",\\n      description: \\"Script for making Claude use uv package manager instead of pip\\"\\n    },\\n    {\\n      url: \\"https://conductor.build/\\",\\n      title: \\"Running multiple Claude Code sessions in parallel\\",\\n      description: \\"Using git worktrees for parallel Claude Code sessions\\"\\n    },\\n    {\\n      url: \\"https://www.reddit.com/r/ClaudeAI/comments/1loodjn/claude_code_now_supports_hooks/\\",\\n      title: \\"Claude Code Hooks Discussion\\",\\n      description: \\"Reddit discussion about Claude Code hooks support\\"\\n    }\\n  ]}\\n/>\\n\\n\\n---\\n\\n*This revolution moves fast. By the time you read this, someone\'s already teaching Claude Code to do things we haven\'t imagined yet.*\\n\\n**Further Reading:**\\n- [Context Engineering by Manus](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) \\n- [Hrishi\'s Claude Code Analysis](https://southbridge-research.notion.site/claude-code-an-agentic-cleanroom-analysis)\\n- [Claude Code Sub-Agents Documentation](https://docs.anthropic.com/en/docs/claude-code/sub-agents)\\n- [Claude Code Hooks Guide](https://docs.anthropic.com/en/docs/claude-code/hooks-guide)\\n\\n---\\n\\n## Are Your Developers Working FOR Claude Code? Or Is Claude Code Working FOR Them?\\n\\nRight now, your engineers spend 30% of their AI time being Claude\'s personal assistants - explaining context, re-describing architecture, and hand-holding every request. **Meanwhile, Anthropic\'s teams reduced research time by 80% and debug 3x faster** because Claude Code works FOR them.\\n\\n**The Reality Check:** Your team bought the most powerful coding AI ever built, then turned themselves into its unpaid interns.\\n\\n**The Transformation:** Stop being Claude\'s employee. Make Claude Code your team\'s autonomous coding partner that knows your codebase better than your junior developers.\\n\\n**Flip the Script: Make Claude Code Work FOR Your Team**\\n\\nTransform your developers from AI babysitters into AI commanders:\\n- Setting up CLAUDE.md files that eliminate context re-explaining  \\n- Building verifiable workflows that turn debugging from hours to minutes\\n- Token optimization strategies that double your effective usage\\n- Advanced automation that makes Claude Code your team\'s autonomous teammate\\n\\n*Read how [Anthropic\'s teams achieve these results](https://www.anthropic.com/news/how-anthropic-teams-use-claude-code) and learn to implement the same strategies for your startup.*\\n\\n<CalendlyButton \\n  url=\\"https://calendly.com/abhishek-vertexcover/claude-code\\"\\n  text=\\"Book Your 30-Minute Consultation Call\\"\\n  description=\\"Transform from Claude Code user to Claude Code teacher. Your future autonomous development workflow starts with one conversation.\\"\\n/>"},{"id":"ai-agent-end-to-end-automated-tests-mobile","metadata":{"permalink":"/ai-agent-end-to-end-automated-tests-mobile","source":"@site/blog/2025-07-10-flowtest/index.mdx","title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","description":"AI for End-to-End Tests (Mobile too!)","date":"2025-07-10T00:00:00.000Z","tags":[{"inline":false,"label":"AI Agents","permalink":"/tags/ai-agents","description":"AI Agents"},{"inline":false,"label":"End-to-End Test","permalink":"/tags/end-to-end-test","description":"End-to-End Test"}],"readingTime":3.67,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"ai-agent-end-to-end-automated-tests-mobile","title":"AI for End-to-End Tests (Mobile too!) with Auto Healing","description":"AI for End-to-End Tests (Mobile too!)","date":"2025-07-10T00:00:00.000Z","tags":["ai-agents","end-to-end-testing"]},"unlisted":false,"prevItem":{"title":"Teaching Claude Code to Work Independently","permalink":"/claude-code-context-engineering-v2"}},"content":"### AI Agent for End-to-End Testing to Deliver Flawless Digital Experiences\\n\\n---\\n\\nWhat if Ai Agent could write tests for your codebase? End-to-end? and for mobile too? and it auto heals / auto-adjusts when your codebase changes?\\n\\nWe share nuggets we learnt while building an AI Agent to solve one of the most persistent challenges in software development: making UI test automation accessible, reliable, and scalable across platforms and devices.\\n\\n{/* truncate */}\\n\\n\\n### The Problem\\n\\nTest automation has historically been:\\n\\n- **Too technical**: requiring code expertise\\n- **Time-consuming**: for authoring and maintaining scripts\\n- **Platform-limited**: with fragmented support for web vs. mobile\\n- **Fragile**: breaking with minor UI changes or incomplete user flows\\n\\nExisting tools were not built for the demands of today\'s fast-moving, multi-platform development cycles. They struggled particularly with hybrid apps, dynamic interfaces, and gesture-driven experiences.\\n\\n---\\n\\n### The Solution: AI Agent\\n\\nAI agent designed from the ground up as an intelligent, prompt-driven automation system with key capabilities:\\n\\n### Natural Language to Automation Code\\n\\nUsers describe test scenarios in plain English. It translates them into precise, executable test scripts\u2014including test data, validations, and edge cases.\\n\\n![flow-diagram](flow-diagram.png)\\n\\n### Web & Mobile App Testing\\n\\nSupports both web (Selenium, Playwright) and mobile (Appium) frameworks, making it one of the few solutions that bridges the gap between platforms seamlessly.\\n\\n### Multi-Language Support\\n\\nGenerates scripts in Java, Python, JavaScript, and other frameworks\u2014tailored to the team\'s existing tech stack.\\n\\n### Smart Debugging\\n\\nExecutes each script line in real-time as it\'s generated, identifying and correcting issues on the fly.\\n\\n### Cross-Device Execution\\n\\nRun tests instantly across 5,000+ combinations of real browsers and devices (when connected to cloud infrastructure).\\n\\n### Self-Healing Automation\\n\\nDetects and updates selectors and steps automatically as the application evolves, eliminating the need for manual maintenance.\\n\\n---\\n\\n### Core Technical Challenges Solved\\n\\n### UI Automation for Flutter Web and Hybrid Mobile Apps\\n\\nMost automation tools break down on platforms like Flutter Web, where the UI is rendered inside a `<canvas>` instead of standard HTML elements, and on hybrid apps without accessible DOM trees.\\n\\n**Our agent solved this by enabling interaction with non-standard UIs using a combination of visual, contextual, and heuristic techniques**\u2014delivering true end-to-end automation where no other solution worked.\\n\\n---\\n\\n### Accurate and Reusable Script Generation\\n\\nRunning LLMs for every test execution is expensive and error-prone.\\n\\nThis AI Agent implemented a **novel templating and generation system** that decouples script generation from execution. This allowed:\\n\\n- Complete and correct scripts on the first pass\\n- Reusability across test runs and frameworks\\n- Fast, low-cost, scalable test creation\\n\\n---\\n\\n### Complex Gestures and UI Behaviors\\n\\nSimulating gestures like pinch, zoom, drag-and-drop, or multi-touch is notoriously hard\u2014especially in custom components.\\n\\nThis agent provided **fine-grained control** over gesture simulation, going beyond the abstractions of typical libraries, enabling accurate testing of sliders, carousels, maps, and more.\\n\\n---\\n\\n### Mapping Natural Language to UI Actions Reliably\\n\\nNatural language like \u201cClick the Pay button\u201d can be ambiguous\u2014especially in large, dynamic UIs.\\n\\n**Being very smart, it combined multiple modalities\u2014DOM structure, visual layout, and semantic cues**\u2014to reliably identify elements even when conventional locators failed. This enabled it to handle vague prompts and incomplete context with high precision.\\n\\n---\\n\\n### Stability in Dynamic & Incomplete User Flows\\n\\nIn real-world apps, pop-ups appear unexpectedly, elements load asynchronously, and flows can be interrupted.\\n\\nIt was was designed to **recover intelligently** from such cases using retry logic, timeout strategies, and partial flow handling. This brought production-grade resilience to end-to-end test execution.\\n\\n---\\n\\n### Scalable Evaluation & Debugging Infrastructure\\n\\nA major limitation of LLM-based systems is the lack of robust evaluation.\\n\\nAI Agent addressed this by building a **custom evaluation framework** that:\\n\\n- Validated script correctness at each step\\n- Enabled partial re-execution of scripts\\n- Provided fine-grained feedback for model improvement\\n    \\nThis drastically accelerated iteration speed and allowed for deeper validation of system accuracy.\\n    \\n\\n---\\n\\n### Conclusion\\n\\nAI Agent redefines what\'s possible in test automation. By combining the reasoning power of LLMs with robust engineering for execution, gesture control, and UI resilience, it makes test automation accessible to non-engineers while retaining power for experts.\\n\\nFrom tackling the hardest UI platforms like Flutter Web to enabling precise, reusable test generation and execution at scale, our custom AI agent is a leap forward in the world of quality engineering.\\n\\nIt\'s not just a tool\u2014it\'s a full-stack AI agent that understands, adapts, and evolves with your application."}]}}')}}]);