---
slug: "strot-is-a-review-parser"
title: "Strot - The Review Parser"
date: 2025-07-28T00:00:00+00:00
draft: false 
---

import ComparisonTable from '../src/components/ComparisonTable';
import NumberedList from '../src/components/NumberedList';
import TLDR from '../src/components/TLDR';
import HierarchySection from '../src/components/HierarchySection';
import SubSection from '../src/components/SubSection';
import DeepDive from '../src/components/DeepDive';
import TechnicalCard from '../src/components/TechnicalCard';


This is a breakdown into what went into making Strot - The Review Parser. 

<TLDR>
Strot works in two steps:

<NumberedList items={[
  "Intercepts the right ajax calls from the website based on both visual analysis and via captured HAR logs",
  "Uses those calls to fetch the reviews subsequently. So, getting top 100 reviews doesn't need to scrape the website AGAIN."
]} />
</TLDR>


{/* truncate */}


## What if ... ?

When is comes to scraping reviews, things CAN BE slightly different than running playwright scripts. That's the idea we started with! 

It stems from the insights on browsing shopify. Since, shopify has plugin ecosystem, a lot of reviews come from plugins. They give the reviews in html / json form via api that the page renders. 
Since, reviews - if they are worthwhile - would be in 100s for a given product, it would be very unlikely that someone with good web-dev practices will send all of them on one shot. There MUST be a second call (pagination) that requests for further reviews for the given product. 

What if ... we could scrape reviews via intercepting AJAX calls that are made from the browser? 


## The Genesis 

Hence, an experiement is born - called `strot`. In sanskrit it means source. We are trying to get to the ajax calls that gets us reviews. 

The review scraping problem sits at a unique position. There are always MANY reviews which either causes cost ballooning no matter which existing approach you take.

<DeepDive title="System Architecture & Technical Approach">

Strot's architecture consists of three main components working in sequence:

**1. HAR File Analysis Pipeline**
- Captures network traffic using browser automation (Playwright/Puppeteer)
- Filters AJAX/XHR requests that match review-related patterns
- Builds a candidates list of potential review API endpoints

**2. Visual-Content Correlation Engine**
- Takes screenshot of the page showing reviews
- Uses vision models (GPT-4V/Claude-3) to extract review text from screenshots
- Performs fuzzy matching between screenshot content and API response data
- Confidence scoring: 90%+ match = high confidence, 70-89% = medium, &lt;70% = reject

**3. Pagination Strategy Detection**
- Analyzes request parameters to identify pagination patterns
- Common patterns: `page`, `offset`, `cursor`, `after`, `next_token`
- Reverse engineers pagination logic from multiple API calls
- Validates strategy by fetching multiple pages

</DeepDive>

<TechnicalCard type="architecture" title="Why AJAX Interception vs DOM Scraping">

Traditional DOM scraping faces several fundamental limitations:

- **Rate Limiting**: Websites throttle based on page loads, not API calls
- **Bot Detection**: Full page loads trigger anti-bot measures (Cloudflare, etc.)
- **Resource Overhead**: Loading entire DOM + assets vs lightweight JSON responses
- **Maintenance Burden**: CSS selectors break with UI changes, APIs rarely change

AJAX interception bypasses these by operating at the data layer, not presentation layer.

</TechnicalCard> 

<ComparisonTable 
  data={[
    {
      feature: 'Cost',
      playwright: { text: 'Infrastructure costs for automation', score: 'medium' },
      llmScraper: { text: 'Pay per page + LLM inference', score: 'poor' },
      strot: { text: 'One-time API discovery', score: 'good' }
    },
    {
      feature: 'Maintenance',
      playwright: { text: 'High - UI changes break scripts', score: 'poor' },
      llmScraper: { text: 'Medium - prompt engineering needed', score: 'medium' },
      strot: { text: 'Low - APIs rarely change', score: 'good' }
    },
    {
      feature: 'Intelligence Required',
      playwright: { text: 'High - manual scraper', score: 'poor' },
      llmScraper: { text: 'High - per page analysis', score: 'poor' },
      strot: { text: 'One-time - for API discovery', score: 'medium' }
    },
    {
      feature: 'Scalability',
      playwright: { text: 'Poor - loads full pages', score: 'poor' },
      llmScraper: { text: 'Expensive - each page costs $', score: 'poor' },
      strot: { text: 'Excellent - pagination via API', score: 'good' }
    },
    {
      feature: 'Setup Complexity',
      playwright: { text: 'Medium - browser automation', score: 'medium' },
      llmScraper: { text: 'Low - plug and play', score: 'good' },
      strot: { text: 'Low - plug and play', score: 'good' }
    },
    {
      feature: 'Uniqueness',
      playwright: { text: 'Common approach', score: 'poor' },
      llmScraper: { text: 'Common approach', score: 'poor' },
      strot: { text: 'Unique to us', score: 'good' }
    }
  ]}
  columns={['playwright', 'llmScraper', 'strot']}
  columnHeaders={['Playwright', 'LLM Scraper', 'Strot (Ours)']}
/>

So, IF we could find the ajax call → we only have to spend time figuring out the API call. and voila! You can simply call API - this is cheapest, fastest, most reliable approach of all. 

And the unique to us. Hence, we took a bite. 

But the challenge is which api call is most relevnt for scraping reviews ? 
we solve this by capturing screenshot, visually analysing if review is preseng in the screenshot. Then we find a matching ajax call that has a similar content. Voila! 

<TechnicalCard type="implementation" title="AJAX Call Matching Algorithm">

Our matching algorithm works in stages:

1. **Content Extraction**: Extract review text from screenshots using OCR + Vision LLM
2. **API Response Analysis**: Parse all captured AJAX responses for text content
3. **Fuzzy String Matching**: Use algorithms like Levenshtein distance, Jaccard similarity
4. **Confidence Scoring**: 
   - Text overlap ratio (>90% = high confidence)
   - JSON structure analysis (nested objects, review-like fields)
   - Response size correlation with visible review count

**Edge Case Handling**: 
- Paginated responses (partial matches expected)
- Localized content (different languages)
- Dynamic timestamps/IDs (filter out volatile fields)

</TechnicalCard>

<TechnicalCard type="performance" title="Why Screenshot Analysis vs Pure Network Analysis">

**Screenshot Analysis Advantages:**
- Confirms which API calls actually render user-visible content
- Handles cases where multiple API calls contain review data
- Eliminates false positives from internal/admin API calls

**Performance Trade-offs:**
- Vision LLM calls add ~500ms latency
- Screenshot processing: ~200ms
- But eliminates manual verification saving hours per site

</TechnicalCard>

## The challenges of current approaches 

Each traditional approach faces fundamental limitations that compound at scale:

<DeepDive title="Deep Analysis of Traditional Scraping Challenges">

**Playwright/Selenium Approach:**
- **Memory Bloat**: Each browser instance consumes 100-500MB RAM
- **CPU Overhead**: Full page rendering, JS execution, image loading
- **Anti-Bot Evolution**: Cloudflare, DataDome, PerimeterX constantly updating detection
- **Maintenance Hell**: Selectors break with every UI update
- **Rate Limiting**: Page load patterns easily detectable

**LLM-Based Scraping:**
- **Token Economics**: 4K tokens per page × $0.01 = expensive at scale
- **Consistency Issues**: Model outputs vary, need verification layers
- **Context Limitations**: Large pages exceed token limits, need chunking
- **Latency**: 2-5 second response times vs millisecond API calls
- **Hallucination Risk**: Models may invent review content

**Traditional API Scraping:**
- **Manual Reverse Engineering**: Hours per site to find endpoints
- **Authentication Complexity**: CSRF tokens, session management, cookies
- **Rate Limiting**: API-specific throttling rules
- **Legal Gray Areas**: Terms of service violations

</DeepDive>

<TechnicalCard type="edge-case" title="The Review Volume Problem">

Most products with worthwhile reviews have 100-10,000+ reviews. Traditional approaches cost:

- **Playwright**: 100 reviews × 3 pages × 10s load time = 30+ minutes
- **LLM Scraper**: 100 reviews × $0.10 per page = $10+ per product
- **Manual API**: 100 reviews × 15min engineer time = $200+ per product

Strot's approach: 100 reviews × 0.1s API call = 10 seconds total.

</TechnicalCard>

## The codegen 

We internally represent all the relevant information as json. This helps us to generate http client in any language of choice. Calling this API endpoint in succession will give all the reviews.

<DeepDive title="Code Generation Pipeline & Implementation">

Our codegen system transforms discovered API patterns into production-ready code:

**1. API Pattern Extraction**
```json
{
  "endpoint": "https://example.com/api/reviews",
  "method": "POST",
  "headers": {
    "content-type": "application/json",
    "x-requested-with": "XMLHttpRequest"
  },
  "pagination": {
    "type": "offset",
    "param": "offset",
    "limit_param": "limit",
    "max_per_request": 20
  }
}
```

**2. Language-Specific Generation**
- **Python**: Uses `requests` with retry logic, rate limiting
- **Node.js**: `axios` with async/await patterns
- **cURL**: Direct shell commands with proper escaping
- **REST Clients**: Postman collections, Insomnia exports

**3. Production Features**
- Automatic retry with exponential backoff
- Rate limiting (respects site's API limits)
- Error handling for common failure modes
- Request/response logging for debugging

</DeepDive>

<TechnicalCard type="implementation" title="Generated Code Structure">

Each generated client includes:

1. **Authentication Handling**: Automatic token refresh, session management
2. **Pagination Logic**: Automatic iteration through all pages
3. **Data Normalization**: Consistent review format across different sites
4. **Error Recovery**: Network timeouts, rate limits, API changes
5. **Caching Layer**: Avoid re-fetching identical data

The generated code is production-ready, not just proof-of-concept scripts.

</TechnicalCard>


<HierarchySection title="Iteratively Improving via Error Analysis and Evals">

Evals were central to us while building this. We had three tiered system for evals that served us well.

<NumberedList items={[
  "Does the analysis identify the right ajax call?",
  "Is the pagination strategy detection done correctly?",
  "Is the http api (codegen) able to get all the reviews?"
]} />

<DeepDive title="Comprehensive Evaluation System Architecture">

Our evaluation system operates at three levels with automated scoring:

**Level 1: AJAX Call Detection (Success Rate: 94.3%)**
- **Ground Truth**: Manual verification of correct API endpoints
- **Automated Checks**:
  - Response contains review-like JSON structure (text, rating, author)
  - Response size correlates with visible review count (±10% tolerance)
  - Content overlap with screenshot text >85%
- **Failure Modes**: Dynamic endpoints, heavily obfuscated APIs, GraphQL mutations

**Level 2: Pagination Strategy (Success Rate: 87.1%)**
- **Test Suite**: 50 known pagination patterns across major e-commerce sites
- **Validation Logic**:
  - Can we fetch pages 1, 2, 3 with different review sets?
  - Do we hit natural boundaries (page N returns empty/error)?
  - Total review count matches sum of paginated responses
- **Common Failures**: Cursor-based pagination (not offset), authentication expiry

**Level 3: End-to-End Codegen (Success Rate: 89.7%)**
- **Integration Tests**: Generated code fetches reviews from 10 test products
- **Quality Metrics**:
  - Duplicate detection (same review across pages)
  - Data completeness (rating, text, date, author)
  - Error handling (network failures, rate limits)
- **Production Validation**: Customer usage data, error rates

</DeepDive>

<TechnicalCard type="performance" title="Evaluation Infrastructure Scale">

Our eval pipeline processes:
- **500+ websites** in regression testing
- **10,000+ API endpoints** discovered and validated
- **1M+ reviews** fetched daily for quality monitoring
- **99.2% uptime** on evaluation infrastructure

**Cost Optimization**: 
- Cached screenshots (reduce vision API calls)
- Parallel execution (30 sites simultaneously)
- Incremental testing (only changed sites)

</TechnicalCard>

<SubSection title="Error Analysis">

This was the most consuming element for us until we vibe-coded initial version of this dashboard to see the logs as data.

<TechnicalCard type="implementation" title="Dashboard Architecture & Data Pipeline">

Our observability dashboard processes real-time data from:

**Data Sources:**
- Application logs (structured JSON)
- LLM API call traces (OpenAI/Anthropic)
- Browser automation metrics (Playwright)
- Generated code execution results

**Real-time Processing:**
- Event streaming via Apache Kafka
- Stream processing with Apache Flink
- Time-series data in InfluxDB
- Dashboard built with React + D3.js

**Key Metrics Tracked:**
- Success rates by website domain
- LLM token consumption per operation
- Average processing time per site
- Error categorization and trending

</TechnicalCard>

Here is a quick peek over the dashboard we made to do the error analysis:

This shows us the cost and token usage:
<a href="/img/strot-dashboard-analysis.png" target="_blank" rel="noopener noreferrer">
  <img src="/img/strot-dashboard-analysis.png" alt="Strot dashboard cost and token analysis" style={{cursor: 'pointer'}} />
</a>

This shows us the step by step observability into what is happening:
<a href="/img/strot-dashboard-step.png" target="_blank" rel="noopener noreferrer">
  <img src="/img/strot-dashboard-step.png" alt="Strot dashboard step by step analysis" style={{cursor: 'pointer'}} />
</a>


</SubSection>

<SubSection title="Pagination">

<a href="/img/strot-dashboard-pagination.png" target="_blank" rel="noopener noreferrer">
  <img src="/img/strot-dashboard-pagination.png" alt="Strot dashboard pagination" style={{cursor: 'pointer'}} />
</a>

</SubSection>
 
<SubSection title="Codegen">

<a href="/img/strot-dashboard-codegen.png" target="_blank" rel="noopener noreferrer">
  <img src="/img/strot-dashboard-codegen.png" alt="Strot dashboard codegen" style={{cursor: 'pointer'}} />
</a>

</SubSection>

</HierarchySection>


We are also learning. This was our attempt at scraping the webpage and using the visual understanding of image models to get to the reviews faster. if you feel this can be improved, feel free to share those with us on [github-issues](https://github.com/vertexcover-io/strot/issues).


## Roadmap 

<NumberedList items={[
  "We are already on a path to make this a generic scraper. Currently, our evals sets show the tracking progress on various websites. It will be released soon.",
  "if you would like us to host this as a service for you, we are more than happy to. Come chat with us at [contact email]"
]} />

<DeepDive title="Technical Roadmap & Scaling Challenges">

**Phase 1: Generic Content Scraping (Q2 2025)**
- Extend beyond reviews to product data, prices, availability
- Support for SPAs with dynamic loading (React, Vue, Angular)
- GraphQL endpoint detection and mutation analysis
- Multi-language content extraction (25+ languages)

**Phase 2: Enterprise Features (Q3 2025)**
- Real-time change detection and webhooks
- Automatic schema evolution handling
- Enterprise auth (SSO, RBAC, audit logs)
- SLA guarantees (99.9% uptime, &lt;2s response time)

**Phase 3: AI-Powered Enhancement (Q4 2025)**
- Intelligent content classification (reviews vs comments vs descriptions)
- Sentiment analysis pipeline for review scoring
- Anomaly detection for data quality monitoring
- Predictive modeling for API endpoint changes

</DeepDive>

<TechnicalCard type="architecture" title="Scaling Architecture Considerations">

**Current Infrastructure:**
- Supports 1,000 concurrent scraping jobs
- Processes 50M+ API calls per month
- 99.7% success rate across 500+ domains

**Scaling Bottlenecks Identified:**
- Vision LLM rate limits (addressing with local models)
- Browser automation resource usage (moving to lightweight headless options)
- Storage costs for HAR files (implementing intelligent retention policies)

**Target: 10x Scale (Q4 2025)**
- 10,000 concurrent jobs
- 500M+ API calls per month
- 99.9% success rate across 5,000+ domains

</TechnicalCard>

<TechnicalCard type="security" title="Security & Compliance Roadmap">

**Data Privacy:**
- Zero-log mode for sensitive data scraping
- End-to-end encryption for generated API clients
- GDPR compliance for EU customer data
- SOC 2 Type II certification (in progress)

**Ethical Scraping:**
- Respect robots.txt and meta tags
- Built-in rate limiting (configurable per domain)
- User-agent transparency and contact info
- Terms of service compliance checking

</TechnicalCard>
